{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTG0qgk9JQF0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PIZmCIpUJj_Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "from numpy import mean\n",
    "from numpy import cov\n",
    "from numpy.linalg import eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zw6rpZH9pWI4"
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8DLmahqFXox"
   },
   "source": [
    "## We read the files using OpenCV and store them in a list after downscaling the images and flattening them. We also create a dictionary mapping the file labels to the image matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = []\n",
    "path= '/content/drive/My Drive/SMAI_Assignment3_Dataset/dataset/'\n",
    "data_path = os.path.join(path,'*g')\n",
    "files = glob.glob(data_path)\n",
    "index = path.rindex('/')\n",
    "labels = []\n",
    "# print(index)\n",
    "labels_dict = {}\n",
    "for f1 in files:\n",
    "    # print(f1)\n",
    "    file_name = f1[index+1:]\n",
    "    file_label, name = file_name.split('_')\n",
    "    labels.append(file_label)\n",
    "    img = cv2.imread(f1)\n",
    "    img = rgb2gray(img)\n",
    "    img = cv2.resize(img, (100,100), interpolation = cv2.INTER_AREA)\n",
    "    face = img.flatten()\n",
    "    faces.append(face)\n",
    "    if file_label in labels_dict:\n",
    "        labels_dict[file_label].append(face)\n",
    "    else:\n",
    "        labels_dict[file_label] = [face] \n",
    "\n",
    "faces = np.array(faces)\n",
    "print(faces)\n",
    "faces.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUK2lPZXGp8o"
   },
   "source": [
    "### We calculate the mean of each column and center the values in each column of the matrix by subtracting the mean column value. We also find the covariance matrix of the centered matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15Ire7mUlFiT"
   },
   "outputs": [],
   "source": [
    "img_mean = mean(faces)\n",
    "temp = faces - img_mean\n",
    "covariance = cov(temp.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzCoKKFYHeOR"
   },
   "source": [
    "### We calculate the eigen decomposition of the covariance matrix, thus resulting in a list of eigenvalues and a list of eigenvectors. We sort the eigen values in descending order and accordingly store the eigen vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Nsj_gW0EBj_"
   },
   "outputs": [],
   "source": [
    "e, v = eig(covariance)\n",
    "indexes = e.argsort()[::-1]   \n",
    "eigen_values = e[indexes]\n",
    "eigen_vectors = v[:,indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ECWLoU52H5t6"
   },
   "source": [
    "### We calculate the number of components required by keeping a variance of atleast 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e7vQj8hcGRTU",
    "outputId": "298f9250-75e1-45cf-9331-20eb3ccfb06f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of components required to achieve less than 10% error:  76\n"
     ]
    }
   ],
   "source": [
    "total = np.sum(eigen_values)\n",
    "variance_reqd = 0.90\n",
    "var_list = list()\n",
    "no_of_components = 0\n",
    "components_reqd = 0\n",
    "sum1 = 0\n",
    "while True:\n",
    "  sum1 += eigen_values[no_of_components]\n",
    "  var = sum1/total\n",
    "  var_list.append(var)\n",
    "  var_achieved = np.real(var)\n",
    "  if var_achieved >= variance_reqd:\n",
    "      components_reqd = no_of_components + 1\n",
    "      break\n",
    "  no_of_components += 1\n",
    "\n",
    "\n",
    "print(\"No. of components required to achieve less than 10% error: \", components_reqd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HIsFB1JCbMGJ"
   },
   "source": [
    "### We perform PCA on the number of components as found out from above and then form the reconstruction matrix, which will be used to reconstruct the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "qiWE2GEVE3vG",
    "outputId": "4f086789-9517-420b-cdd4-cb98c825f4ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(520, 10000)\n",
      "(520, 10000)\n"
     ]
    }
   ],
   "source": [
    "components = components_reqd\n",
    "\n",
    "vectors = eigen_vectors[:,:components]\n",
    "transformation_matrix = np.real(vectors)\n",
    "pca_projections = np.dot(faces, transformation_matrix)\n",
    "reconstruction_matrix = np.dot(pca_projections, transformation_matrix.T)\n",
    "\n",
    "print(faces.shape)\n",
    "print(reconstruction_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ptTD8gq90yf"
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "labels_dict1 = {}\n",
    "for i in range(reconstruction_matrix.shape[0]):\n",
    "    face = reconstruction_matrix[i]\n",
    "    if labels[i] in labels_dict1:\n",
    "        labels_dict1[labels[i]].append(face)\n",
    "    else:\n",
    "        labels_dict1[labels[i]] = [face] \n",
    "\n",
    "faces = np.array(faces)\n",
    "# print(faces)\n",
    "# faces.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rDN5Fltbt9M"
   },
   "source": [
    "## We create the training and testing sets by splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fn1hDO9N_SI5"
   },
   "outputs": [],
   "source": [
    "num_of_classes = len(labels_dict1)\n",
    "\n",
    "def one_hot_encoding(label):\n",
    "  one_hot_label = []\n",
    "  one_hot_label = [0 for i in range(num_of_classes)]\n",
    "  one_hot_label[label] = 1\n",
    "  return one_hot_label\n",
    "\n",
    "X_train = [0 for i in range(10000)]\n",
    "y_train = [0 for i in range(8)]\n",
    "\n",
    "X_test = [0 for i in range(10000)]\n",
    "y_test = []\n",
    "\n",
    "for key,value in labels_dict1.items():\n",
    "  label = int(key)\n",
    "  one_hot_label = one_hot_encoding(label)\n",
    "  data = np.array(value)\n",
    "  train = data[:55,:]\n",
    "  test = data[55:,:]\n",
    "  X_train = np.vstack((X_train, train))\n",
    "  X_test = np.vstack((X_test, test))\n",
    "  y_train = np.vstack((y_train,np.array([one_hot_label]*55)))\n",
    "  for i in range(10):\n",
    "    y_test.append(label)\n",
    "\n",
    "X_train = X_train[1:,:]\n",
    "y_train = y_train[1:,:]\n",
    "X_test = X_test[1:,:]\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3ZC34uibkqN"
   },
   "source": [
    "## We scale the data and then add the bias column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMUTPmC2_YWB"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "scaler.fit(X_test)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7D0Ga6RjK1yZ"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape[0])\n",
    "ones = np.ones([X_train.shape[0],1])\n",
    "X_train = np.concatenate((ones,X_train),axis=1)\n",
    "ones = np.ones([X_test.shape[0],1])\n",
    "X_test = np.concatenate((ones,X_test),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M143I2SZMQkE"
   },
   "source": [
    "## Logistic regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpeKu76z_e5A"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def loss_func(h, y):\n",
    "  return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "\n",
    "def logistic_regression(X_train, y_train, iter, alpha):\n",
    "  theta = np.random.rand(X_train.shape[1])\n",
    "  count = 1\n",
    "  while(count<=iter):\n",
    "    temp = np.dot(X_train, theta)\n",
    "    h = sigmoid(temp)\n",
    "    # print(h.dtype)\n",
    "    count = count+1\n",
    "\n",
    "    error = h-y_train\n",
    "\n",
    "\n",
    "    gradient = np.dot(X_train.T, error)/y_train.shape[0]\n",
    "    theta = theta - alpha*gradient\n",
    "    # print(count)\n",
    "    # z = np.dot(X_train, theta)\n",
    "    # h = sigmoid(z)\n",
    "    # loss = loss_func(h, y_train)\n",
    "\n",
    "    # print(loss)\n",
    "\n",
    "  return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5fbU8kzMPmE"
   },
   "source": [
    "## We run the logistic regression function using learning rate as 0.001 for 10000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Vo4yRKV_mhQ"
   },
   "outputs": [],
   "source": [
    "weights = [[0 for i in range(10001)]]\n",
    "for i in range(num_of_classes):\n",
    "  weights = np.vstack((weights,logistic_regression(X_train, y_train[:,i], 10000, 0.001)))\n",
    "\n",
    "\n",
    "weights = weights[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5d20ZM4dXdT"
   },
   "source": [
    "## We make the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5w_R8l_KdXdZ"
   },
   "outputs": [],
   "source": [
    "predictions = sigmoid(np.dot(X_test, weights.T))\n",
    "print(predictions)\n",
    "print(predictions.shape)\n",
    "\n",
    "y_pred=[]\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "  maxElement = np.amax(predictions[i])\n",
    "  for j in range(8):\n",
    "    if(predictions[i][j] == maxElement):\n",
    "      ind = j\n",
    "      break\n",
    "  y_pred.append(j)\n",
    "\n",
    "# print(len(y_pred))\n",
    "# print(y_pred)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vv2j1Tv9dXd2"
   },
   "source": [
    "## The accuracy is found out to be 0.65\n",
    "### Due the random initialization of weights, the accuracy may vary slightly in different trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0j1Lr2FYfRbs",
    "outputId": "f2b12785-141f-4e27-8eaa-2ed91e7a70b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.65\n"
     ]
    }
   ],
   "source": [
    "a_1 = (y_pred == y_test).mean()\n",
    "print(\"Accuracy: \", a_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4X52wIT9jSOZ"
   },
   "source": [
    "## The confusion matrix and f1 score obtained are printed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "wilF_ebwesOK",
    "outputId": "12134ec1-3261-49b3-b002-0967d61193ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  2  0  0  2  0  0  1]\n",
      " [ 2  3  1  2  1  0  1  0]\n",
      " [ 0  1  5  2  0  1  1  0]\n",
      " [ 0  0  0 10  0  0  0  0]\n",
      " [ 0  0  1  0  8  1  0  0]\n",
      " [ 0  0  2  0  0  8  0  0]\n",
      " [ 2  2  1  1  0  0  4  0]\n",
      " [ 0  0  0  0  0  1  0  9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "han0vrh9e1RD",
    "outputId": "514de375-c646-41bd-c61a-cd0ff8b1c041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.6354323308270677\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_1 = f1_score(y_test,y_pred, average='weighted')\n",
    "print(\"f1 score: \", f1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFjftTri_tJO"
   },
   "source": [
    "# Without PCA\n",
    "## In this section we test our model without applying PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RtCCtj3XLAI"
   },
   "source": [
    "## First we create the training and testing sets by splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gVeeXxp6uAU"
   },
   "outputs": [],
   "source": [
    "num_of_classes = len(labels_dict)\n",
    "\n",
    "def one_hot_encoding(label):\n",
    "  one_hot_label = []\n",
    "  one_hot_label = [0 for i in range(num_of_classes)]\n",
    "  one_hot_label[label] = 1\n",
    "  return one_hot_label\n",
    "\n",
    "X_train = [0 for i in range(10000)]\n",
    "y_train = [0 for i in range(8)]\n",
    "\n",
    "X_test = [0 for i in range(10000)]\n",
    "y_test = []\n",
    "\n",
    "for key,value in labels_dict.items():\n",
    "  label = int(key)\n",
    "  one_hot_label = one_hot_encoding(label)\n",
    "  # print(label, one_hot_label)\n",
    "  data = np.array(value)\n",
    "  train = data[:55,:]\n",
    "  test = data[55:,:]\n",
    "  X_train = np.vstack((X_train, train))\n",
    "  X_test = np.vstack((X_test, test))\n",
    "  y_train = np.vstack((y_train,np.array([one_hot_label]*55)))\n",
    "  for i in range(10):\n",
    "    y_test.append(label)\n",
    "\n",
    "X_train = X_train[1:,:]\n",
    "y_train = y_train[1:,:]\n",
    "X_test = X_test[1:,:]\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4osLmRKqXXVl"
   },
   "source": [
    "## We scale the data and then add the bias column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5QF0SbH3kvx"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "scaler.fit(X_test)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyJUhsHnWiPa"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape[0])\n",
    "ones = np.ones([X_train.shape[0],1])\n",
    "X_train = np.concatenate((ones,X_train),axis=1)\n",
    "ones = np.ones([X_test.shape[0],1])\n",
    "X_test = np.concatenate((ones,X_test),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EJkvCfiLXcQn"
   },
   "source": [
    "## Logistic regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWfaiEDg0-Fd"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def loss_func(h, y):\n",
    "  return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "\n",
    "def logistic_regression(X_train, y_train, iter, alpha):\n",
    "  theta = np.random.rand(X_train.shape[1])\n",
    "  count = 1\n",
    "  while(count<=iter):\n",
    "    temp = np.dot(X_train, theta)\n",
    "    h = sigmoid(temp)\n",
    "    # print(h.dtype)\n",
    "    count = count+1\n",
    "\n",
    "    error = h-y_train\n",
    "\n",
    "\n",
    "    gradient = np.dot(X_train.T, error)/y_train.shape[0]\n",
    "    theta = theta - alpha*gradient\n",
    "    # print(count)\n",
    "    # z = np.dot(X_train, theta)\n",
    "    # h = sigmoid(z)\n",
    "    # loss = loss_func(h, y_train)\n",
    "\n",
    "    # print(loss)\n",
    "\n",
    "  return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KaxPpRPOYkiT"
   },
   "source": [
    "## We run the logistic regression function using learning rate as 0.01 for 10000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mh-B2OCy6dPM"
   },
   "outputs": [],
   "source": [
    "weights = [[0 for i in range(10001)]]\n",
    "for i in range(num_of_classes):\n",
    "  weights = np.vstack((weights,logistic_regression(X_train, y_train[:,i], 10000, 0.01)))\n",
    "\n",
    "\n",
    "weights = weights[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIqWkPBVYmjM"
   },
   "source": [
    "## We make the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2J-0_YABAMAi"
   },
   "outputs": [],
   "source": [
    "predictions = sigmoid(np.dot(X_test, weights.T))\n",
    "print(predictions)\n",
    "print(predictions.shape)\n",
    "\n",
    "y_pred=[]\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "  maxElement = np.amax(predictions[i])\n",
    "  for j in range(8):\n",
    "    if(predictions[i][j] == maxElement):\n",
    "      ind = j\n",
    "      break\n",
    "  y_pred.append(j)\n",
    "\n",
    "# print(len(y_pred))\n",
    "# print(y_pred)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMQTkTX4fpoj"
   },
   "source": [
    "## The accuracy is found out to be 0.65\n",
    "### Due the random initialization of weights, the accuracy may vary slightly in different trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9XiC1Nrgfpo0",
    "outputId": "96b86045-6047-4c6a-a5e4-d439afba8d16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6875\n"
     ]
    }
   ],
   "source": [
    "a_2 = (y_pred == y_test).mean()\n",
    "print(\"Accuracy: \", a_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MzacZoiNjcYw"
   },
   "source": [
    "## The confusion matrix and f1 score obtained are printed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "PjDhEeJjfppR",
    "outputId": "ea4c9187-5889-4002-9bd9-9e61f0c8c68c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 0 1 1 1 0 0 0]\n",
      " [1 6 0 1 0 0 2 0]\n",
      " [1 0 8 0 0 0 1 0]\n",
      " [0 2 0 7 0 1 0 0]\n",
      " [0 1 0 0 4 4 1 0]\n",
      " [0 0 2 0 0 8 0 0]\n",
      " [0 1 2 0 0 0 6 1]\n",
      " [0 0 0 0 0 1 0 9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LcP_Y4J8fppj",
    "outputId": "b933b328-3f5f-4d83-d8fb-b7101eea7472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.6836670480549198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_2 = f1_score(y_test,y_pred, average='weighted')\n",
    "print(\"f1 score: \", f1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EW2OtBmZfhxD"
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "colab_type": "code",
    "id": "diBs8FAygXd7",
    "outputId": "f2066f17-437b-433f-ae3d-69b6ae3f87cf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>f1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model without PCA</td>\n",
       "      <td>0.6500</td>\n",
       "      <td>0.635432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model without PCA</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.683667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model  Accuracy  f1 score\n",
       "0  Model without PCA    0.6500  0.635432\n",
       "1  Model without PCA    0.6875  0.683667"
      ]
     },
     "execution_count": 164,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_pca = [\"Model without PCA\", a_1, f1_1]\n",
    "with_pca = [\"Model without PCA\", a_2, f1_2]\n",
    "data = [without_pca, with_pca]\n",
    "df = pd.DataFrame(data, columns = ['Model', 'Accuracy', 'f1 score'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qjGypgGkY-Q_"
   },
   "source": [
    "## Thus we see that our model performed relatively well on applying PCA, getting an accuracy score comparable to that of the model where PCA is not applied."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of assign3q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
